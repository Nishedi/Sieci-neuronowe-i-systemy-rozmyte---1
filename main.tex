\documentclass[12pt]{article}

\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{newclude}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{tikz}
 \usepackage{url}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{subcaption}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{minted}
\usepackage{tabularx}
\lstdefinestyle{mystyle}{
    belowcaptionskip=1\baselineskip,
    frame=single, 
    frameround=tttt,
    xleftmargin=\parindent,
    language=[x86masm]Assembler,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\itshape\color{green!60!black},
    keywordstyle=\color{blue!80!black},
    identifierstyle=\color{red!80!black},
    tabsize=4,
    numbers=left,
    numbersep=8pt,
    stepnumber=1,
    numberstyle=\tiny\color{gray}, 
    columns = fullflexible,
}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{setspace} % Pakiet do ustawiania interlinii
\onehalfspacing
\begin{document}

\begin{titlepage}
		\begin{figure}[h]
			\begin{minipage}[l]{.5\textwidth}%
				\includegraphics[width=0.3\textwidth]{pwr_logo}
			\end{minipage}%
			\begin{minipage}[r]{.5\textwidth}%
				\includegraphics[width=1\textwidth]{wit_logo}
			\end{minipage}%
		\end{figure}
		
		\vspace*{3mm}
		
		\begin{center}
			\rule{\textwidth}{0.8pt}\\ 
			\vspace*{6mm}
			{\LARGE \textbf{Sieci neuronowe i systemy rozmyte}\\
            
            \vspace*{6mm}
            
            Sieci płytkie }\\
            \vspace*{3mm}
			\rule{\textwidth}{0.8pt}\\
			
			\vspace{1.5cm}
			{\setstretch{2}
				Politechnika Wrocławska
				
				Wydział Informatyki i Telekomunikacji
				
				Kierunek: Informatyczne Systemy Automatyki
				
				Grupa nr 1
				
				
			}
		\end{center}
		
		\vspace*{2cm}
		
		\begin{flushright}
			{\setstretch{2}
            	Konrad Pempera - $263948$\\
	
                
				\textbf{Termin zajęć}: Poniedziałek godz. $11^{\underline{15}}$ - $13^{\underline{00}}$ 
				
				\textbf{Prowadzący:} Dr inż. Piotr Ciskowski
				
			}
			
		\end{flushright}
		
		\vfill
		
\end{titlepage}

\tableofcontents
\clearpage
\section*{Sprawozdanie z realizacji projektu: Sieci Płytkie}

\subsection*{1. Cel projektu}
Celem projektu było kompleksowe zbadanie procesu uczenia dwuwarstwowych sieci neuronowych (sieci płytkich) poprzez:
\begin{enumerate}
\item Implementację dwuwarstwowej sieci neuronowej z propagacją wsteczną (backpropagation)
\item Nauczenie sieci na klasycznym problemie XOR (nieliniowo separowalnym)
\item Systematyczne zbadanie wpływu różnych technik optymalizacji:
    \begin{itemize}
    \item Momentum
    \item Adaptacyjny współczynnik uczenia (adaptive learning rate)
    \item Uczenie mini-batch
    \item Wczesne zatrzymanie (early stopping)
    \end{itemize}
\item Wizualizację procesu uczenia poprzez wykresy:
    \begin{itemize}
    \item Błędu MSE w obu warstwach sieci
    \item Błędu klasyfikacji
    \item Ewolucji wag w czasie
    \end{itemize}
\item Przetestowanie sieci na rzeczywistym problemie klasyfikacyjnym z ML Repository (Breast Cancer)
\item Przeprowadzenie analizy porównawczej skuteczności różnych konfiguracji uczenia
\end{enumerate}

\subsection*{2. Implementacja sieci}

\subsubsection*{2.1. Architektura sieci}
Zaimplementowana sieć składała się z:
\begin{itemize}
\item \textbf{Warstwy wejściowej} -- liczba neuronów odpowiada liczbie cech (2 dla XOR, 30 dla Breast Cancer)
\item \textbf{Warstwy ukrytej} -- konfigurowalna liczba neuronów (4 dla XOR, 16 dla Breast Cancer) z funkcją aktywacji sigmoid
\item \textbf{Warstwy wyjściowej} -- 1 neuron z funkcją sigmoid dla klasyfikacji binarnej
\end{itemize}

\subsubsection*{2.2. Funkcje aktywacji}
Zastosowano funkcję sigmoid w obu warstwach:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
Jej pochodna, niezbędna w algorytmie backpropagation:
\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]

\subsubsection*{2.3. Funkcja kosztu}
Użyto błędu średniokwadratowego (Mean Squared Error):
\[
MSE = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{2}(y_i - \hat{y}_i)^2
\]
gdzie $y_i$ to wartość oczekiwana, a $\hat{y}_i$ to predykcja sieci.

\subsubsection*{2.4. Algorytm uczenia -- Backpropagation}
Uczenie odbywało się przy użyciu metody spadku gradientu z propagacją wsteczną błędu:

\textbf{Faza forward:}
\begin{align*}
z^{(1)} &= X \cdot W^{(1)} + b^{(1)} \\
h &= \sigma(z^{(1)}) \\
z^{(2)} &= h \cdot W^{(2)} + b^{(2)} \\
\hat{y} &= \sigma(z^{(2)})
\end{align*}

\textbf{Faza backward:}
\begin{align*}
\delta^{(2)} &= (\hat{y} - y) \odot \sigma'(z^{(2)}) \\
\frac{\partial L}{\partial W^{(2)}} &= h^T \cdot \delta^{(2)} \\
\delta^{(1)} &= (\delta^{(2)} \cdot (W^{(2)})^T) \odot \sigma'(z^{(1)}) \\
\frac{\partial L}{\partial W^{(1)}} &= X^T \cdot \delta^{(1)}
\end{align*}

\textbf{Aktualizacja wag:}
\begin{align*}
W^{(l)} &\leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}
\end{align*}
gdzie $\eta$ to współczynnik uczenia (learning rate).

\subsection*{3. Dodatkowe mechanizmy optymalizacji}
W projekcie zaimplementowano następujące rozszerzenia podstawowego algorytmu spadku gradientu:

\subsubsection*{3.1. Momentum}
Momentum dodaje składową bezwładności do aktualizacji wag, wykorzystując informację z poprzednich kroków:
\begin{align*}
v_t &= \gamma v_{t-1} - \eta \nabla L(W_t) \\
W_{t+1} &= W_t + v_t
\end{align*}
gdzie:
\begin{itemize}
\item $v_t$ -- wektor prędkości (velocity) w iteracji $t$
\item $\gamma$ -- współczynnik momentum (typowo 0.9)
\item $\eta$ -- współczynnik uczenia
\item $\nabla L(W_t)$ -- gradient funkcji kosztu
\end{itemize}

\textbf{Zalety:}
\begin{itemize}
\item Przyspiesza konwergencję w kierunkach o konsystentnym gradiencie
\item Tłumi oscylacje w kierunkach o zmiennym gradiencie
\item Pomaga w przezwyciężeniu lokalnych minimów
\item Redukuje wrażliwość na szum w gradientach
\end{itemize}

\subsubsection*{3.2. Adaptacyjny learning rate}
Współczynnik uczenia jest automatycznie dostosowywany na podstawie postępu uczenia:
\[
\eta_{t+1} = \begin{cases}
1.1 \cdot \eta_t & \text{jeśli } MSE_t < MSE_{t-1} \text{ (postęp)} \\
0.9 \cdot \eta_t & \text{jeśli } MSE_t \geq MSE_{t-1} \text{ (brak postępu)}
\end{cases}
\]
z ograniczeniami: $0.01 \leq \eta_t \leq 1.0$

\textbf{Zalety:}
\begin{itemize}
\item Automatyczne dostosowanie tempa uczenia bez ręcznego doboru
\item Większe kroki na początku uczenia (szybka eksploracja)
\item Mniejsze kroki przy zbliżaniu się do minimum (precyzyjne dostrojenie)
\item Redukcja ryzyka "przeskoczenia" optimum
\end{itemize}

\subsubsection*{3.3. Mini-batch learning}
Zamiast aktualizacji wag po każdym przykładzie (online) lub po całym zbiorze (batch), gradient jest obliczany na małej porcji danych:
\[
\nabla L(W) \approx \frac{1}{|B|} \sum_{i \in B} \nabla L_i(W)
\]
gdzie $B$ to mini-batch o rozmiarze $|B|$ (typowo 16-128 przykładów).

\textbf{Zalety:}
\begin{itemize}
\item Uśrednianie gradientów redukuje szum charakterystyczny dla uczenia online
\item Szybsza konwergencja niż w przypadku pełnego batch learning
\item Lepsze wykorzystanie pamięci podręcznej procesora
\item Możliwość równoległego przetwarzania na GPU
\item Regularyzacyjny efekt -- szum pomaga w unikaniu przeuczenia
\end{itemize}

\subsubsection*{3.4. Early stopping}
Uczenie jest przerywane, gdy błąd na zbiorze treningowym osiągnie zadany próg:
\[
\text{stop if } MSE_{train} \leq MSE_{target}
\]

\textbf{Zalety:}
\begin{itemize}
\item Oszczędność czasu obliczeniowego
\item Zapobieganie przeuczeniu (overfitting)
\item Automatyczne zakończenie po osiągnięciu wystarczającej jakości
\end{itemize}

\subsection*{4. Eksperyment dla problemu XOR}
Problem XOR jest nieliniowo separowalny, co oznacza, że pojedynczy perceptron nie jest w stanie go rozwiązać. Dwuwarstwowa sieć z warstwą ukrytą umożliwia nauczenie tego rozkładu.

W ramach eksperymentów przeprowadzono serię testów z różnymi konfiguracjami uczenia, aby zbadać wpływ poszczególnych technik optymalizacji na proces uczenia sieci. Każda konfiguracja została zademonstrowana przy użyciu trzech rodzajów wykresów:
\begin{itemize}
\item błędu MSE na zbiorze uczącym oraz pełnym zbiorze (z podziałem na warstwy),
\item błędu klasyfikacji przy progu 0.5,
\item ewolucji wag w obu warstwach sieci.
\end{itemize}

\subsubsection*{4.1. Uczenie podstawowe (bez optymalizacji)}
Pierwszym eksperymentem było uczenie sieci bez żadnych dodatkowych mechanizmów optymalizacji. Parametry uczenia:
\begin{itemize}
\item Współczynnik uczenia: 0.3
\item Batch size: 1 (online learning)
\item Momentum: 0.0 (wyłączone)
\item Adaptacyjny learning rate: wyłączony
\item Early stopping: wyłączony
\item Liczba epok: 5000
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – mse_.png}
    \caption{Błąd średniokwadratowy MSE w czasie uczenia -- wersja podstawowa. Górny wykres pokazuje MSE w warstwie ukrytej (warstwa 1), dolny wykres pokazuje MSE zarówno na przykładach uczących (niebieski), jak i na całym zbiorze uczącym (czerwony). Widoczne są duże oscylacje błędu, charakterystyczne dla uczenia online bez mechanizmów stabilizujących.}
    \label{fig:mse_basic}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{XOR – classification_error_.png}
    \caption{Błąd klasyfikacji w czasie uczenia -- wersja podstawowa. Błąd maleje stopniowo, ale w sposób niemonotyczny. Sieć osiąga 0\% błędu klasyfikacji po około 2500 epokach, choć proces jest niestabilny.}
    \label{fig:class_error_basic}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – weights_.png}
    \caption{Ewolucja wag w obu warstwach sieci -- wersja podstawowa. Górny wykres przedstawia wagi pierwszej warstwy (wejście $\rightarrow$ warstwa ukryta), dolny wykres pokazuje wagi drugiej warstwy (warstwa ukryta $\rightarrow$ wyjście). Widoczne są duże oscylacje wartości wag przez cały proces uczenia.}
    \label{fig:weights_basic}
\end{figure}

\textbf{Obserwacje:} Uczenie podstawowe charakteryzuje się dużą niestabilnością. Błąd MSE wykazuje silne oscylacje, co jest typowe dla uczenia online z małym współczynnikiem uczenia. Proces wymaga dużej liczby epok do osiągnięcia konwergencji, a wagi nie stabilizują się w sposób gładki.

\subsubsection*{4.2. Uczenie z early stopping}
Drugim eksperymentem było dodanie mechanizmu wczesnego zatrzymania uczenia (early stopping), który przerywa proces po osiągnięciu zadanego poziomu błędu MSE. Parametry uczenia:
\begin{itemize}
\item Współczynnik uczenia: 0.3
\item Batch size: 1 (online learning)
\item Momentum: 0.0 (wyłączone)
\item Adaptacyjny learning rate: wyłączony
\item Early stopping: włączony (target MSE = 0.001)
\item Maksymalna liczba epok: 5000
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – mse_Early stopping.png}
    \caption{Błąd średniokwadratowy MSE w czasie uczenia z early stopping. Uczenie zostało przerwane po osiągnięciu docelowego błędu MSE = 0.001. Widoczne jest znaczne skrócenie czasu uczenia w porównaniu do wersji podstawowej.}
    \label{fig:mse_early}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{XOR – classification_error_Early stopping.png}
    \caption{Błąd klasyfikacji w czasie uczenia z early stopping. Sieć osiąga 0\% błędu klasyfikacji znacznie wcześniej niż w wersji podstawowej, a uczenie jest automatycznie przerywane po osiągnięciu zadanego kryterium.}
    \label{fig:class_error_early}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – weights_Early stopping.png}
    \caption{Ewolucja wag w obu warstwach sieci z early stopping. Proces uczenia jest krótszy, co widać po mniejszej liczbie punktów na wykresie. Wagi osiągają wartości pozwalające na prawidłową klasyfikację bez zbędnego przedłużania procesu uczenia.}
    \label{fig:weights_early}
\end{figure}

\textbf{Obserwacje:} Mechanizm early stopping znacząco skraca czas uczenia poprzez zatrzymanie procesu po osiągnięciu zadanego poziomu błędu. Zapobiega to niepotrzebnemu przedłużaniu uczenia i potencjalnym problemom z przeuczeniem. W przypadku problemu XOR uczenie zostało przerwane około 1500 epoki, oszczędzając znaczną część czasu obliczeniowego.

\subsubsection*{4.3. Uczenie z momentum}
Trzecim eksperymentem było dodanie mechanizmu momentum, który przyspiesza uczenie poprzez akumulację poprzednich kroków gradientu. Parametry uczenia:
\begin{itemize}
\item Współczynnik uczenia: 0.3
\item Batch size: 1 (online learning)
\item Momentum: 0.9
\item Adaptacyjny learning rate: wyłączony
\item Early stopping: włączony (target MSE = 0.001)
\item Maksymalna liczba epok: 5000
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – mse_Momentum.png}
    \caption{Błąd średniokwadratowy MSE w czasie uczenia z momentum. Oscylacje błędu są znacznie mniejsze niż w wersji podstawowej. Momentum pomaga w szybszej konwergencji i stabilizuje proces uczenia.}
    \label{fig:mse_momentum}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{XOR – classification_error_Momentum.png}
    \caption{Błąd klasyfikacji w czasie uczenia z momentum. Proces konwergencji jest płynniejszy i szybszy w porównaniu do poprzednich wersji. Sieć osiąga 0\% błędu klasyfikacji wcześniej i w bardziej stabilny sposób.}
    \label{fig:class_error_momentum}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – weights_Momentum.png}
    \caption{Ewolucja wag w obu warstwach sieci z momentum. Wagi ewoluują w sposób znacznie bardziej gładki niż w poprzednich eksperymentach. Momentum redukuje chaotyczne zmiany wartości wag, prowadząc do stabilniejszej konwergencji.}
    \label{fig:weights_momentum}
\end{figure}

\textbf{Obserwacje:} Wprowadzenie momentum (współczynnik 0.9) znacząco poprawia stabilność procesu uczenia. Oscylacje błędu są wyraźnie mniejsze, a wagi zmieniają się w sposób bardziej gładki. Momentum działa jak "bezwładność" w przestrzeni parametrów, pomagając przezwyciężyć lokalne minima i przyspieszyć konwergencję w kierunku globalnego minimum.

\subsubsection*{4.4. Uczenie z momentum i adaptacyjnym learning rate}
Czwartym eksperymentem było połączenie momentum z adaptacyjnym współczynnikiem uczenia, który automatycznie dostosowuje wielkość kroku w zależności od postępu uczenia. Parametry uczenia:
\begin{itemize}
\item Początkowy współczynnik uczenia: 0.3
\item Batch size: 1 (online learning)
\item Momentum: 0.9
\item Adaptacyjny learning rate: włączony (zwiększa o 10\% przy zmniejszeniu błędu, zmniejsza o 10\% przy wzroście błędu)
\item Early stopping: włączony (target MSE = 0.001)
\item Maksymalna liczba epok: 5000
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – mse_Momentum, adaptive LR.png}
    \caption{Błąd średniokwadratowy MSE w czasie uczenia z momentum i adaptacyjnym learning rate. Połączenie obu technik prowadzi do jeszcze szybszej konwergencji. Adaptacyjny learning rate automatycznie dostosowuje tempo uczenia, co widać po charakterystycznych zmianach nachylenia krzywej błędu.}
    \label{fig:mse_momentum_adaptive}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{XOR – classification_error_Momentum, adaptive LR.png}
    \caption{Błąd klasyfikacji w czasie uczenia z momentum i adaptacyjnym learning rate. Konwergencja do 0\% błędu klasyfikacji następuje bardzo szybko i w sposób monotoniczny. Jest to najszybsza konwergencja spośród wszystkich dotychczasowych konfiguracji.}
    \label{fig:class_error_momentum_adaptive}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – weights_Momentum, adaptive LR.png}
    \caption{Ewolucja wag w obu warstwach sieci z momentum i adaptacyjnym learning rate. Wagi stabilizują się bardzo szybko i gładko. Adaptacyjny learning rate pozwala na agresywniejsze kroki na początku uczenia (gdy błąd jest duży) i bardziej ostrożne dostosowania pod koniec (gdy sieć zbliża się do optimum).}
    \label{fig:weights_momentum_adaptive}
\end{figure}

\textbf{Obserwacje:} Kombinacja momentum i adaptacyjnego learning rate daje najlepsze rezultaty spośród testowanych konfiguracji. Adaptacyjny learning rate automatycznie dostosowuje tempo uczenia -- zwiększa je, gdy sieć robi postępy, i zmniejsza, gdy błąd rośnie. W połączeniu z momentum prowadzi to do bardzo szybkiej i stabilnej konwergencji.

\subsubsection*{4.5. Uczenie z mini-batch, momentum i adaptacyjnym learning rate}
Ostatnim eksperymentem było zastosowanie uczenia mini-batch w połączeniu z momentum i adaptacyjnym learning rate. Parametry uczenia:
\begin{itemize}
\item Początkowy współczynnik uczenia: 0.3
\item Batch size: 2 (mini-batch)
\item Momentum: 0.9
\item Adaptacyjny learning rate: włączony
\item Early stopping: włączony (target MSE = 0.001)
\item Maksymalna liczba epok: 5000
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – mse_mini-batch, momentum, adaptive LR, target MSE.png}
    \caption{Błąd średniokwadratowy MSE w czasie uczenia z mini-batch, momentum i adaptacyjnym learning rate. Mini-batch (rozmiar 2) dodatkowo stabilizuje proces uczenia poprzez uśrednianie gradientów z kilku przykładów. Wykres jest jeszcze gładszy niż w poprzednich konfiguracjach.}
    \label{fig:mse_minibatch}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{XOR – classification_error_mini-batch, momentum, adaptive LR, target MSE.png}
    \caption{Błąd klasyfikacji w czasie uczenia z mini-batch, momentum i adaptacyjnym learning rate. Konwergencja jest bardzo szybka i stabilna. Mini-batch redukuje szum w gradientach, co prowadzi do bardziej przewidywalnego procesu uczenia.}
    \label{fig:class_error_minibatch}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{XOR – weights_mini-batch, momentum, adaptive LR, target MSE.png}
    \caption{Ewolucja wag w obu warstwach sieci z mini-batch, momentum i adaptacyjnym learning rate. Jest to najgładsza ewolucja wag spośród wszystkich testowanych konfiguracji. Mini-batch dodatkowo redukuje szum w aktualizacjach wag poprzez uśrednianie gradientów.}
    \label{fig:weights_minibatch}
\end{figure}

\textbf{Obserwacje:} Zastosowanie mini-batch w rozmiarze 2 (połowa zbioru XOR) w połączeniu z momentum i adaptacyjnym learning rate daje najbardziej stabilny proces uczenia. Gradienty obliczane są jako średnia z kilku przykładów, co redukuje szum charakterystyczny dla uczenia online. W rezultacie otrzymujemy bardzo gładkie krzywe błędu i ewolucji wag, przy zachowaniu szybkiej konwergencji.

\subsection*{4.6. Analiza porównawcza różnych metod optymalizacji}

Poniższa tabela podsumowuje wpływ różnych technik optymalizacji na proces uczenia sieci dla problemu XOR:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Konfiguracja} & \textbf{Epoki do} & \textbf{Stabilność} & \textbf{Szybkość} \\
 & \textbf{konwergencji} & \textbf{uczenia} & \textbf{konwergencji} \\
\hline
Podstawowa (bez opt.) & $\sim$2500 & Niska & Powolna \\
\hline
+ Early stopping & $\sim$1500 & Niska & Powolna \\
\hline
+ Momentum & $\sim$800 & Średnia & Średnia \\
\hline
+ Momentum + Adapt. LR & $\sim$400 & Wysoka & Szybka \\
\hline
+ Mini-batch + Mom. + Adapt. LR & $\sim$300 & Bardzo wysoka & Bardzo szybka \\
\hline
\end{tabular}
\caption{Porównanie różnych konfiguracji uczenia dla problemu XOR}
\label{tab:comparison}
\end{table}

\textbf{Kluczowe wnioski z analizy porównawczej:}
\begin{itemize}
\item \textbf{Momentum} jest najważniejszym pojedynczym ulepszeniem -- redukuje liczbę epok potrzebnych do konwergencji o około 70\% w porównaniu do wersji podstawowej.
\item \textbf{Adaptacyjny learning rate} w połączeniu z momentum dodatkowo przyspiesza uczenie o około 50\%.
\item \textbf{Mini-batch} przede wszystkim stabilizuje proces uczenia, redukując szum w gradientach. Wpływ na szybkość konwergencji jest mniejszy, ale znacząco poprawia gładkość procesu.
\item \textbf{Early stopping} jest użytecznym mechanizmem praktycznym, który zapobiega niepotrzebnemu przedłużaniu uczenia, ale sam w sobie nie poprawia szybkości ani stabilności konwergencji.
\end{itemize}

\subsection*{5. Eksperyment z danymi rzeczywistymi (Breast Cancer)}
Zestaw danych \texttt{breast\_cancer} z biblioteki scikit-learn zawiera 569 przykładów opisujących cechy próbek tkanki piersi. Każdy przykład ma 30 cech numerycznych (np. średnica komórki, symetryczność, tekstura). Zadaniem jest klasyfikacja binarna: nowotwór złośliwy (malignant) vs. niezłośliwy (benign).

\subsubsection*{5.1. Przygotowanie danych}
Dane zostały podzielone na zbiór treningowy (80\%, 455 przykładów) i testowy (20\%, 114 przykładów) z zachowaniem proporcji klas (stratified split). Wszystkie cechy zostały znormalizowane przy użyciu StandardScaler, aby zapewnić podobną skalę wartości.

\subsubsection*{5.2. Architektura sieci}
Ze względu na większą liczbę cech wejściowych (30) użyto większej sieci:
\begin{itemize}
\item Warstwa wejściowa: 30 neuronów (po jednym na każdą cechę)
\item Warstwa ukryta: 16 neuronów z funkcją aktywacji sigmoid
\item Warstwa wyjściowa: 1 neuron z funkcją sigmoid (klasyfikacja binarna)
\end{itemize}

\subsubsection*{5.3. Parametry uczenia}
Sieć została nauczona z następującymi parametrami:
\begin{itemize}
\item Współczynnik uczenia: 0.1
\item Batch size: 32 (mini-batch)
\item Momentum: 0.9
\item Adaptacyjny learning rate: włączony
\item Liczba epok: 200
\end{itemize}

\subsubsection*{5.4. Wyniki}
Po przygotowaniu danych (standaryzacja, podział train/test):
\begin{itemize}
\item Sieć osiągnęła dokładność klasyfikacji na zbiorze treningowym powyżej 95\%
\item Dokładność na zbiorze testowym wyniosła ponad 90\%, co świadczy o dobrej generalizacji
\item Błąd MSE stopniowo malał w trakcie uczenia bez oznak przeuczenia
\item Wagi stabilizowały się wraz z kolejnymi epokami, osiągając stabilne wartości około 100 epoki
\end{itemize}

\textbf{Obserwacje:} Sieć dwuwarstwowa z odpowiednio dobranymi parametrami (mini-batch, momentum, adaptacyjny learning rate) jest w stanie efektywnie uczyć się rzeczywistych problemów klasyfikacyjnych. Zastosowane techniki optymalizacji zapewniają stabilny i szybki proces uczenia. Wysoka dokładność na zbiorze testowym (powyżej 90\%) pokazuje, że sieć dobrze generalizuje na niewidzianych danych.

\subsection*{6. Wnioski końcowe}
Realizacja projektu pozwoliła na kompleksowe zbadanie procesu uczenia dwuwarstwowych sieci neuronowych oraz wpływu różnych technik optymalizacji na ich skuteczność.

\subsubsection*{6.1. Wnioski teoretyczne}
\begin{itemize}
\item Sieci dwuwarstwowe (płytkie) mimo swojej prostoty są wystarczające do rozwiązywania nieliniowo separowalnych problemów, takich jak XOR.
\item Warstwa ukryta z funkcją aktywacji nieliniowej (sigmoid) umożliwia tworzenie złożonych granic decyzyjnych.
\item Problem XOR wymaga co najmniej 2 neuronów w warstwie ukrytej, aby możliwe było jego rozwiązanie.
\end{itemize}

\subsubsection*{6.2. Wnioski praktyczne dotyczące optymalizacji}
\begin{itemize}
\item \textbf{Momentum} jest najważniejszą pojedynczą techniką optymalizacji -- przyspiesza konwergencję o około 70\% i znacząco stabilizuje proces uczenia poprzez akumulację poprzednich kierunków gradientu.
\item \textbf{Adaptacyjny learning rate} w połączeniu z momentum dodatkowo przyspiesza uczenie o około 50\% poprzez automatyczne dostosowywanie tempa uczenia do postępów sieci.
\item \textbf{Mini-batch learning} stabilizuje proces uczenia poprzez uśrednianie gradientów z kilku przykładów, redukując szum charakterystyczny dla uczenia online. Wielkość batcha powinna być dobrana eksperymentalnie -- większe batche dają większą stabilność, ale mniejsze zapewniają lepszą eksplorację przestrzeni parametrów.
\item \textbf{Early stopping} jest użytecznym mechanizmem praktycznym zapobiegającym przeuczeniu i oszczędzającym czas obliczeniowy, ale nie poprawia samego procesu uczenia.
\end{itemize}

\subsubsection*{6.3. Wnioski z eksperymentów}
\begin{itemize}
\item Na problemie syntetycznym (XOR) wszystkie techniki optymalizacji pokazały wyraźną poprawę -- od chaotycznego uczenia podstawowego do stabilnej i szybkiej konwergencji z pełnym zestawem optymalizacji.
\item Na rzeczywistych danych (Breast Cancer) sieć płytka osiągnęła dokładność przekraczającą 90\%, co pokazuje praktyczną użyteczność takich architektur dla problemów o umiarkowanej złożoności.
\item Wykresy MSE, błędu klasyfikacji i wag dostarczają cennych informacji diagnostycznych o procesie uczenia i pomagają w debugowaniu problemów z konwergencją.
\end{itemize}

\subsubsection*{6.4. Rekomendacje}
Dla praktycznych zastosowań sieci płytkich zaleca się:
\begin{enumerate}
\item Zawsze stosować momentum (wartość 0.9 jest dobrym punktem wyjścia)
\item Rozważyć adaptacyjny learning rate dla problemów, gdzie optymalna wartość współczynnika uczenia nie jest znana
\item Używać mini-batch zamiast uczenia online (batch size 16-64 dla małych zbiorów, 128-512 dla większych)
\item Implementować early stopping z odpowiednim kryterium zatrzymania
\item Monitorować wykresy błędu i wag podczas uczenia do wczesnego wykrywania problemów
\end{enumerate}

\subsubsection*{6.5. Podsumowanie}
Projekt wykazał, że nawet proste sieci płytkie mogą być bardzo efektywne przy odpowiednim doborze technik optymalizacji. Kombinacja momentum, adaptacyjnego learning rate i mini-batch learning prowadzi do procesu uczenia, który jest jednocześnie szybki, stabilny i skuteczny. Techniki te stanowią fundament dla bardziej zaawansowanych algorytmów optymalizacji używanych w głębokich sieciach neuronowych.

\end{document}